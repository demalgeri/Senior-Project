# -*- coding: utf-8 -*-
"""Capstone Assignment 14 Combined.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nxtk6HPv7bm3sxwe2ghex283hy9aPiIl
"""

'''
Task: Stock market prediction with classification -> Predict whether the prices will rise or fall in the next day.
Companies: Apple and Procter & Gamble
Models: SVM, KNN, DT, LR, XGB Classifier, RNN, CNN, FFNN
Features: use historical stock market data to calculate handcrafted features and raw featuers: Average Volume, Volatility, 
          Average Volatility, Company, and closing price
Dataset: yfinance module
Implementation:
  1) Prepare the dataframe
  2) Train the models
  3) Make predictions
'''

#Load the dataset
!pip install yfinance

#Import statements
import yfinance as yf
import pandas as pd
import matplotlib.pyplot as plt
import math
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split 
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix as cmatrix
import numpy as np
import warnings
warnings.filterwarnings("ignore")
from sklearn.metrics import plot_confusion_matrix
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.layers import Flatten
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.preprocessing import MinMaxScaler
from datetime import timedelta
import datetime
from sklearn.utils import resample
from tensorflow import keras
from keras.models import Sequential
from keras import layers
from keras.layers import LSTM, Dropout, Dense, Conv1D, MaxPooling1D
from collections import deque
import random
from keras.callbacks import TensorBoard, ModelCheckpoint
import time
from itertools import chain
from sklearn.metrics import accuracy_score
from sklearn.utils import shuffle 
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
import seaborn as sns

###########################
#Data Preprocessing
###########################
#Split the data into train, validation, and test sets
#64% train, 16% validation, 20% test
def split_data(data, companies):
  #Declare the data sets
  train_data = pd.DataFrame()
  valid_data = pd.DataFrame()
  test_data = pd.DataFrame()

  #Calculate the values of where to split the data
  tot_len = len(data)
  train_len = int(tot_len * 0.64) 
  valid_len = train_len + int(tot_len * 0.16)

  #Get the train, validation, and test data, then randomly shuffle it
  train_data = data.iloc[0:train_len]
  valid_data = data.iloc[train_len:valid_len]
  test_data = data.iloc[valid_len:]

  train_data = shuffle(train_data)
  valid_data = shuffle(valid_data)
  test_data = shuffle(test_data)

  #Now split into X and y
  #The feature columns are the X data, and the label column is the y data
  X_train = pd.DataFrame()
  y_train = pd.DataFrame()
  X_valid = pd.DataFrame()
  y_valid = pd.DataFrame()
  X_test = pd.DataFrame()
  y_test = pd.DataFrame()

  X_train = train_data[['Close', 'Avg Volume', 'Volatility', 'Avg Volatility', 'Company']]
  y_train = train_data['Rise']

  X_valid = valid_data[['Close', 'Avg Volume', 'Volatility', 'Avg Volatility', 'Company']]
  y_valid = valid_data['Rise']

  X_test = test_data[['Close', 'Avg Volume', 'Volatility', 'Avg Volatility', 'Company']]
  y_test = test_data['Rise']

  return X_train, y_train, X_valid, y_valid, X_test, y_test
#########################################################################################################################
#Normalize the data - Avg Volume is the only one that needs to be normalized
def normalize_data(train, valid, test):
  #Create temporary dataframes for the train and valid datasets
  train_norm = pd.DataFrame()
  train_norm['Avg Volume'] = train['Avg Volume']

  valid_norm = pd.DataFrame()
  valid_norm['Avg Volume'] = valid['Avg Volume']

  test_norm = pd.DataFrame()
  test_norm['Avg Volume'] = test['Avg Volume']

  #define scaler
  scaler = MinMaxScaler()
  #fit only on the train dataset
  scaler = scaler.fit(train_norm) 

  #transform the train dataset
  train_norm = scaler.transform(train_norm)
  #Put the transformed data into a new dataframe column
  train_norm = list(train_norm)
  tn = pd.DataFrame()
  tn['Avg Volume'] = train_norm

  #transform the valid dataset with the scaler that is fitted with the train dataset
  valid_norm = scaler.transform(valid_norm)
  #Put the transformed data into a new dataframe column
  valid_norm = list(valid_norm)
  vn = pd.DataFrame()
  vn['Avg Volume'] = valid_norm

  #transform the test dataset with the scaler that is fitted with the train dataset
  test_norm = scaler.transform(test_norm)
  #Put the transformed data into a new dataframe column
  test_norm = list(test_norm)
  ten = pd.DataFrame()
  ten['Avg Volume'] = test_norm

  #Put the normalized data into the original dataframes
  train['Avg Volume'] = tn['Avg Volume'].values
  valid['Avg Volume'] = vn['Avg Volume'].values
  test['Avg Volume'] = ten['Avg Volume'].values

  train['Avg Volume'] = train['Avg Volume'].str.get(0)
  valid['Avg Volume'] = valid['Avg Volume'].str.get(0)
  test['Avg Volume'] = test['Avg Volume'].str.get(0)

  return train, valid, test
#########################################################################################################################
#Calculate average volume for a feature
def averageVolume(df_col):
  volumes = list(df_col)

  stock_volume = list()
  s_vol = 0
  prices = list(df_col)
  for i in range(len(prices)): #First range parameter is 6 because we don't want to include 2018-01-01
    #average volume over the previous 5 days
    s_vol = volumes[i-5] + volumes[i-4] + volumes[i-3] + volumes[i-2] + volumes[i-1]
    s_vol = s_vol / 5
    stock_volume.append(s_vol)

  return stock_volume
#########################################################################################################################
#Calculate volatilty for a feature
def calc_volatility(df_col):
  prices = list(df_col)
  volatility = list()
  vol = 0

  for i in range(len(prices)):
    vol = prices[i-1] - prices[i]
    vol = vol / prices[i-1]
    volatility.append(vol)
    
  return volatility
#########################################################################################################################
#Calculate Stock Price Volatility for a feature- It is an average of previous 5 days volatility of a company. 
def calc_stock_price_volatility(df_col):
  prices = list(df_col)
  volatility = calc_volatility(df_col)

  stock_price_volatility = list()
  sp_vol = 0

  for i in range(len(prices)): 
    sp_vol = volatility[i-5] + volatility[i-4] + volatility[i-3] + volatility[i-2] + volatility[i-1]
    sp_vol = sp_vol / 5
    stock_price_volatility.append(sp_vol)

  return stock_price_volatility
#########################################################################################################################
#Function to get the previous 5 days from the start date in order to calculate the handcrafted features
def get_adjusted_start_date(sd, interv, days_before):
  x = sd.split('-')
  year = int(x[0])
  month = int(x[1])
  day = int(x[2])
  x = datetime.datetime(year, month, day)
  if(interv == '1d'):
    date = x - timedelta(days=days_before)
  elif(interv == '1wk'):
    date = x - timedelta(weeks=days_before)
  else:
    date = x - timedelta(months=days_before)

  date = str(date)
  date = date.split(' ')
  date = date[0]

  return date
#########################################################################################################################
#Function to get one day in the future from the end date in order to calculate the labels
def get_adjusted_end_date(sd, interv, days_ahead):
  x = sd.split('-')
  year = int(x[0])
  month = int(x[1])
  day = int(x[2])
  x = datetime.datetime(year, month, day)
  if(interv == '1d'):
    date = x + timedelta(days=days_ahead)
  elif(interv == '1wk'):
    date = x + timedelta(weeks=days_ahead)
  else:
    date = x + timedelta(months=days_ahead)

  date = str(date)
  date = date.split(' ')
  date = date[0]

  return date
#########################################################################################################################
#This function takes the arguments that the user passed for the companies, start date, end date, and interval
#But gets 5 datapoints before the start date and 1 datapoint before the end date.
#This is because we need the extra datapoints in order to calculated the handcrafted features and the labels.
#These newly added datapoints will get thrown out after the features and labels are calculated.
def get_adjusted_dataframe(companies, sd, ed, interv):
  #Download the historical data based on the parameters
  data = yf.download(tickers=companies, start=sd, end=ed, interval=interv)

  #Download one future day as well
  orig_len = len(data)
  days_ahead = 1
  new_ed = ed
  while(len(data) < orig_len + 1):
    data = pd.DataFrame()
    new_ed = get_adjusted_end_date(new_ed, interv, days_ahead)
    data = yf.download(tickers=companies, start=sd, end=new_ed, interval=interv)

  #Download the previous 5 days as well
  orig_len = len(data)
  days_before = 1
  new_sd = sd
  while(len(data) < orig_len + 5): 
    data = pd.DataFrame()
    new_sd = get_adjusted_start_date(new_sd, interv, days_before)
    data = yf.download(tickers=companies, start=new_sd, end=new_ed, interval=interv)

  return data
#########################################################################################################################
#This function calculates the 'Company' feature and the 'Rise' label.
#It also counts how many rise and fall data points there are.
def get_comp_and_lab(data, companies):
  #Declare lists to store all the information
  company = list()
  label = list()
  coms = list()
  lab = list()

  #This variable will distinguish the companies from each other
  j = 0

  #These variables will let us know which class is the majority and minority
  rise = 0
  fall = 0

  for ticker in companies:
    data = data.drop(columns=[('Adj Close', ticker), ('High', ticker), ('Low', ticker), ('Open', ticker)])

    #Create the label column and the company column
    coms = list()
    lab = list()
    y = list(data[('Close', ticker)].pct_change())
    y.pop(0)
    for i in range(len(y)):
      num = str(y[i]) #get the percent change as a string in order to tell if it is negative or positive.
      coms.append(j) #save the company
      if num[0] == '-':
        lab.append(0) #save the label, 0 = Fall
        fall += 1 #sum of the number of fall data points
      elif num[0] != '-':
        lab.append(1) #save the label, 1 = Rise
        rise += 1 #sum of the number of rise data points
    lab.append(np.nan) #append nan to the end of labels because we are going to drop the last day from the dataframe
    coms.append(j) #append one more to the end of coms so that the length is the same as the lab length (we will drop last element of each company anyway)
    j += 1
    data[('Company', ticker)] = coms
    data[('Rise', ticker)] = lab

  return data, rise, fall
#########################################################################################################################
def get_more_features(data, companies):
  for ticker in companies:
    volatility = list()
    stock_price_volatility = list()
    avg_volume = list()

    #Volatility
    df_col = data[('Close', ticker)]
    volatility = calc_volatility(df_col)
    #Stock Price Volatility
    stock_price_volatility = calc_stock_price_volatility(data[('Close', ticker)])
    #Average Stock Volume
    avg_volume = averageVolume(data[('Volume', ticker)])

    data[('Avg Volume', ticker)] = avg_volume
    data[('Volatility', ticker)] = volatility
    data[('Avg Volatility', ticker)] = stock_price_volatility

  return data
#########################################################################################################################
#This function combines both tickers so that the different pieces of information from each company can be in one 
#column in the dataframe, rather than each company have it's own columns
def combine_tickers(data, companies):
  close = list()
  avg_volume = list()
  volatility = list()
  avg_volatility = list()
  company = list()
  rise = list()
  idx = list()

  for ticker in companies:
    #Get a list of the date and ticker so that we can rename the index of the dataframe
    for i in range(len(data.index)):
      new_str = str(data.index[i])
      new_new_str = new_str.split(' ')
      new_index = new_new_str[0] + "_" + ticker
      idx.append(new_index)
  
    #Save the information in lists that will be added to a new dataframe
    for i in data[('Close', ticker)]:
      close.append(i) #Save all closing prices
    for i in data[('Avg Volume', ticker)]:
      avg_volume.append(i) #Save all the average volumes
    for i in data[('Volatility', ticker)]:
      volatility.append(i) #Save all the volatilities
    for i in data[('Avg Volatility', ticker)]:
      avg_volatility.append(i) #Save all the average volatilities  
    for i in data[('Company', ticker)]:
      company.append(i)
    for i in data[('Rise', ticker)]:
      rise.append(i)

  #Create a new dataframe
  df = pd.DataFrame()
  df['Close'] = close
  df['Avg Volume'] = avg_volume
  df['Volatility'] = volatility
  df['Avg Volatility'] = avg_volatility
  df['Company'] = company
  df['Rise'] = rise

  #rename the index numbers to be the date/company
  for i in range(len(df)):
    df.rename(index={i:idx[i]}, inplace=True)

  return df
#########################################################################################################################
#This function drops the excess fall or rise data points in the data set so that there are an equal number of both
def downsample_data(data, rise, fall, companies):
  #Downsample the majority class if the split is greater than 51%-49% so that there will be an equal 
  #(or close to equal) number of Fall and Rise data points
  df_new = pd.DataFrame()
  tot_samples = rise + fall
  if ((rise/tot_samples) > 0.51):
    if (rise < fall):
      df_min = data[data.Rise == 1]
      df_maj = data[data.Rise == 0]
    else:
      df_min = data[data.Rise == 0]
      df_maj = data[data.Rise == 1]

    df_maj_down = resample(df_maj, replace = False, n_samples = len(df_min), random_state = 0)
    df_new = pd.concat([df_min,df_maj_down])
  else:
    df_new = data

  df_new = df_new.sort_index()

  return df_new
#########################################################################################################################
#Fetch the historical stock market data from the yfinance module and manipulate it into handcrafted features
#You can adjust: companies, start date, end date, and time interval
def get_Features(companies, sd, ed, interv):
  #Create an empty dataframe
  data = pd.DataFrame()
  data = get_adjusted_dataframe(companies, sd, ed, interv)
  #Drop the empty cells in the dataframe
  data = data.dropna()
  #Get average volume, volatility and average volatility
  data = get_more_features(data, companies)
  #Get the information for the company feature and the labels and also count how many rise and fall data points there are
  data, rise, fall = get_comp_and_lab(data, companies)

  #Delete the last row
  data = data.drop(data.index[-1])
  #Delete the first 5 rows
  for i in range(5):
    data = data.drop(data.index[0])

  #Drop Close and Volume columns
  for ticker in companies:
    data = data.drop(columns=[('Volume', ticker)])

  #Make one column for each feature rather than having seperate columns for each company
  data = combine_tickers(data, companies)

  #Downsample the data to have a balanced number of rise and fall data points
  data = downsample_data(data, rise, fall, companies)

  #64% train, 16% validation, 20% test
  X_train = pd.DataFrame()
  y_train = pd.DataFrame()
  X_valid = pd.DataFrame()
  y_valid = pd.DataFrame()
  X_test = pd.DataFrame()
  y_test = pd.DataFrame()
  X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(data, companies)

  # #Normalize average volume
  # X_train, X_valid, X_test = normalize_data(X_train, X_valid, X_test)

  #Return the newly created dataframe and train/valid/test sets
  return data, X_train, y_train, X_valid, y_valid, X_test, y_test
#########################################################################################################################
#Function to assemble the final dataframe
def getData(companies, sd, ed, interv):
  data = pd.DataFrame()
  X_train = pd.DataFrame()
  y_train = pd.DataFrame()
  X_valid = pd.DataFrame()
  y_valid = pd.DataFrame()
  X_test = pd.DataFrame()
  y_test = pd.DataFrame()

  data, X_train, y_train, X_valid, y_valid, X_test, y_test = get_Features(companies, sd, ed, interv)

  return data, X_train, y_train, X_valid, y_valid, X_test, y_test

companies = ['AAPL', 'PG']
data, X_train, y_train, X_valid, y_valid, X_test, y_test = getData(companies, '1999-12-31', '2020-01-02', "1d")

print("--------dataframe--------\n")
print(data)
print("--------X_train--------\n")
print(X_train)
print("--------y_train--------\n")
print(y_train)
print("--------X_valid--------\n")
print(X_valid)
print("--------y_valid--------\n")
print(y_valid)
print("--------X_test--------\n")
print(X_test)
print("--------y_test--------\n")
print(y_test)
print("# of data set samples:", len(data))
print("# of train set samples:", len(X_train))
print("# of valid set samples:", len(X_valid))
print("# of test set samples:", len(X_test))

###########################
#Train the Models
###########################

#SVM
def trainSVM(X_train_svc, y_train_svc, X_valid_svc, y_valid_svc, X_test_svc, y_test_svc):

  P = [ 'l1', 'l2', 'elasticnet', 'none' ]
  L = [ 'hinge', 'squared_hinge' ]
  C = [1 , 10 , 0.1]

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for p in P: 
    for c in C:
      for l in L:
        try:
          svc_clf = LinearSVC(penalty=p, C=c, loss=l, random_state=123)
          svc_clf.fit(X_train_svc, y_train_svc)
          pred_svc = svc_clf.predict(X_valid_svc)
          results[(p, c, l)] = f1_score(y_valid_svc, pred_svc, average='weighted')
        except:
          continue

  print("SVC Model:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if max_value < current_value:
      max_value = current_value
      p_best = item[0][0]
      c_best = item[0][1]
      l_best = item[0][2]

  print("Best hyperparameters:", p_best, ",", c_best, ",", l_best)

  #Train the model with the best parameters
  svc_clf = LinearSVC(penalty=p_best, C=c_best, loss=l_best, random_state=123)
  svc_clf.fit(X_train_svc, y_train_svc)
  #Do prediction with original test set, not the validation set
  y_pred_svc = svc_clf.predict(X_test_svc)
  print("\nClassification Report:\n", classification_report(y_true=y_test_svc, y_pred=y_pred_svc, digits=4))
  print("\t    SVM Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_svc, y_pred_svc)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainSVM(X_train, y_train, X_valid, y_valid, X_test, y_test)

#KNN
def trainKNN(X_train_knn, y_train_knn, X_valid_knn, y_valid_knn, X_test_knn, y_test_knn):
  N = list(range(1, 12, 2))
  M = ['chebyshev', 'manhattan', 'euclidean']

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for n in N:
    for m in M:
      knn_clf = KNeighborsClassifier(n_neighbors=n, metric=m)
      knn_clf.fit(X_train_knn, y_train_knn)
      pred_knn = knn_clf.predict(X_valid_knn)
      results[(n, m)] = f1_score(y_valid_knn, pred_knn, average='weighted')

  print("KNN Model:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if max_value < current_value:
      max_value = current_value
      n_best = item[0][0]
      m_best = item[0][1]

  print("Best hyperparameters:", n_best,",", m_best)

  #Train the model with the best parameters
  knn_clf = KNeighborsClassifier(n_neighbors=n_best, metric=m_best)
  knn_clf.fit(X_train_knn, y_train_knn)
  #Do prediction with original test set, not the validation set
  y_pred_knn = knn_clf.predict(X_test_knn)
  print("\nClassification Report:\n", classification_report(y_true=y_test_knn, y_pred=y_pred_knn, digits=4))
  print("\t    KNN Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_knn, y_pred_knn)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainKNN(X_train, y_train, X_valid, y_valid, X_test, y_test)

#DT
def trainDT(X_train_dt, y_train_dt, X_valid_dt, y_valid_dt, X_test_dt, y_test_dt):
  C = ['gini', 'entropy']

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for c in C:
      dt_clf = DecisionTreeClassifier(criterion=c) #do I need random_state=123?
      dt_clf.fit(X_train_dt, y_train_dt)
      pred_dt = dt_clf.predict(X_valid_dt)
      results[c] = f1_score(y_valid_dt, pred_dt, average='weighted')

  print("DT Model:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if (max_value < current_value) or (max_value == current_value):
      max_value = current_value
      c_best = item[0]

  print("Best hyperparameter:", c_best)

  # #Train the model with the best parameters
  dt_clf = DecisionTreeClassifier(criterion=c_best) #do I need random_state=123?
  dt_clf.fit(X_train_dt, y_train_dt)
  #Do prediction with original test set, not the validation set
  y_pred_dt = dt_clf.predict(X_test_dt)
  print("\nClassification Report:\n", classification_report(y_true=y_test_dt, y_pred=y_pred_dt, digits=4))
  print("\t    DT Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_dt, y_pred_dt)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainDT(X_train, y_train, X_valid, y_valid, X_test, y_test)

#LR
def trainLR(X_train_lr, y_train_lr, X_valid_lr, y_valid_lr, X_test_lr, y_test_lr):    
  P = [ 'l1', 'l2', 'elasticnet', 'none' ]
  S = [ 'newton-cg', 'lbfgs', 'sag', 'liblinear', 'saga' ]
  C = [1 , 10 , 0.1]

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for p in P:
    for c in C:
      for s in S:
        try:
          lr_clf = LogisticRegression(penalty=p, C=c, solver=s, random_state=123)
          lr_clf.fit(X_train_lr, y_train_lr)
          pred_lr = lr_clf.predict(X_valid_lr)
          results[(p, c, s)] = f1_score(y_valid_lr, pred_lr, average='weighted')
        except:
          continue

  print("LR Model of:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if max_value < current_value:
      max_value = current_value
      p_best = item[0][0]
      c_best = item[0][1]
      s_best = item[0][2]
      
  print("Best hyperparameters:", p_best, ",", c_best, ",", s_best)

  #Train the model with the best parameters
  lr_clf = LogisticRegression(penalty=p_best, C=c_best, solver=s_best, random_state=123)
  lr_clf.fit(X_train_lr, y_train_lr)
  #Do prediction with original test set, not the validation set
  y_pred_lr = lr_clf.predict(X_test_lr)
  print("\nClassification Report:\n", classification_report(y_true=y_test_lr, y_pred=y_pred_lr, digits=4))
  print("\t    LR Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_lr, y_pred_lr)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainLR(X_train, y_train, X_valid, y_valid, X_test, y_test)

#XGB
def trainXGB(X_train_xgb, y_train_xgb, X_valid_xgb, y_valid_xgb, X_test_xgb, y_test_xgb):
  B = ['gbtree', 'gblinear', 'dart'] #booster options
  O = ['reg:squarederror', 'binary:hinge', 'reg:logistic', 'binary:logistic']#Objective defines the loss function to be minimized
  N = [100, 1000] #n_estimators
  L = [0.02, 0.01] #learning rate

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for b in B:
    for o in O:
      for n in N:
        for l in L:
          xgb_clf = XGBClassifier(booster=b, objective=o, n_estimators=n, learning_rate=l)
          xgb_clf.fit(X_train_xgb, y_train_xgb)
          pred_xgb = xgb_clf.predict(X_valid_xgb)
          results[(b, o, n, l)] = f1_score(y_valid_xgb, pred_xgb, average='weighted')

  print("XGB Classifier Model:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if max_value < current_value:
      max_value = current_value
      b_best = item[0][0]
      o_best = item[0][1]
      n_best = item[0][2]
      l_best = item[0][3]

  print("Best hyperparameters:", b_best, ",", o_best, ",", n_best, ",", l_best)

  # #Train the model with the best parameters
  xgb_clf = XGBClassifier(booster=b_best, objective=o_best, n_estimators=n_best, learning_rate=l_best) 
  xgb_clf.fit(X_train_xgb, y_train_xgb)
  #Do prediction with original test set, not the validation set
  y_pred_xgb = xgb_clf.predict(X_test_xgb)
  print("\nClassification Report:\n", classification_report(y_true=y_test_xgb, y_pred=y_pred_xgb, digits=4))
  print("\t    XGB Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_xgb, y_pred_xgb)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainXGB(X_train, y_train, X_valid, y_valid, X_test, y_test)

'''
This function takes in a dataframe that holds the prices and labels of multiple companies.
It returns the X and y values of each company.
The X values are put into sequences of 5 prices each to signify the 5 previous data points.
Then I shuffle the sequences because the sequences hold time information and company information within themselves.
'''
def get_sequences(df, seq_len):
  #holds the first letter of the company name in the dataframe index
  first_company = df.index[0][0]
  #will hold a list of all the features for the first company
  temp_sequential_data_1 = []
  #will hold a list of all the features for the second company
  temp_sequential_data_2 = []
  #will hold a list of all the features for both of the companies
  sequential_data = []
  #will update the sequence of 5 previous days
  prev_days_close_1 = deque(maxlen=seq_len)
  prev_days_close_2 = deque(maxlen=seq_len)
  #holds all sequential X and y values of each company
  closes = []
  avgVols = []
  volas = []
  avgVolas = []
  comps = []
  y = []
  #tells us if we moved on to the next company already
  switched = False 
  #keeps track of the index
  index = 0 

  for i in df[[('Close'), ('Avg Volume'), ('Volatility'), ('Avg Volatility'), ('Company'), ('Rise')]].values:
    if(df.index[index][0] != first_company):
      prev_days_close_2.append(i[0])
      if len(prev_days_close_2) == seq_len:
        temp_sequential_data_2.append([np.array(prev_days_close_2), i[-5], i[-4], i[-3], i[-2], i[-1]])
      switched = True
    else:
      prev_days_close_1.append(i[0])
      if len(prev_days_close_1) == seq_len:
        temp_sequential_data_1.append([np.array(prev_days_close_1), i[-5], i[-4], i[-3], i[-2], i[-1]])
    index = index + 1

  sequential_data = temp_sequential_data_1 + temp_sequential_data_2

  #shuffle the sequential data. The sequences themselves already have time relationship information that the model
  #should be able to pick up on and the order the sequences are placed shouldn't matter.
  random.shuffle(sequential_data)

  for close, avgVol, vola, avgVola, comp, label in sequential_data:
    closes.append(close)
    avgVols.append(avgVol)
    volas.append(vola)
    avgVolas.append(avgVola)
    comps.append(comp)
    y.append(label)

  return np.array(closes), np.array(avgVols), np.array(volas), np.array(avgVolas), np.array(comps), np.array(y)
#########################################################################################################################
def reorder_data(data):
  df = pd.DataFrame()
  df['Close'] = data['Close']
  df['Avg Volume'] = data['Avg Volume']
  df['Volatility'] = data['Volatility']
  df['Avg Volatility'] = data['Avg Volatility']
  df['Company'] = data['Company']
  df['Rise'] = data['Rise']
  index_list = list()

  #rename the index numbers to be the date/company
  for i in range(len(data)):
    s = str(data.index[i])
    new_s = s.split('_')
    new_indx =  new_s[1] + "_" + new_s[0] 
    index_list.append(new_indx)

  for i in range(len(df)):
    df.rename(index={df.index[i]:index_list[i]}, inplace=True)

  df = df.sort_index()
  
  return df
#########################################################################################################################
#Split the data into train, validation, and test sets
#64% train, 16% validation, 20% test
def split_data(data):
  #Declare the data sets
  train_data = pd.DataFrame()
  valid_data = pd.DataFrame()
  test_data = pd.DataFrame()

  #Calculate the values of where to split the data
  tot_len = len(data)
  train_len = int(tot_len * 0.64) 
  valid_len = train_len + int(tot_len * 0.16)

  #Get the train, validation, and test data
  train_data = data.iloc[0:train_len]
  valid_data = data.iloc[train_len:valid_len]
  test_data = data.iloc[valid_len:]

  #Put the data in order chronologically and by company
  train_data = reorder_data(train_data)
  valid_data = reorder_data(valid_data)
  test_data = reorder_data(test_data)

  return train_data, valid_data, test_data

def get_Features(data, seq_len):

  train, valid, test = split_data(data)

  X_train_close, X_train_avgVol, X_train_vola, X_train_avgVola, X_train_comp, y_train = get_sequences(train, seq_len)
  X_valid_close, X_valid_avgVol, X_valid_vola, X_valid_avgVola, X_valid_comp, y_valid = get_sequences(valid, seq_len)
  X_test_close, X_test_avgVol, X_test_vola, X_test_avgVola, X_test_comp, y_test = get_sequences(test, seq_len)

  return (X_train_close, X_train_avgVol, X_train_vola, X_train_avgVola, X_train_comp, y_train, 
          X_valid_close, X_valid_avgVol, X_valid_vola, X_valid_avgVola, X_valid_comp, y_valid,
          X_test_close, X_test_avgVol, X_test_vola, X_test_avgVola, X_test_comp, y_test)

seq_len = 5 #5 days as the window
(X_train_close, X_train_avgVol, X_train_vola, X_train_avgVola, X_train_comp, y_train_2, 
          X_valid_close, X_valid_avgVol, X_valid_vola, X_valid_avgVola, X_valid_comp, y_valid_2,
          X_test_close, X_test_avgVol, X_test_vola, X_test_avgVola, X_test_comp, y_test_2) = get_Features(data, seq_len)

print("LENGTH OF DATA SET:", len(data), "\n")
print("Data:\n", data)

print("--------------\n")

print("LENGTH OF TRAIN SET:", len(y_train_2), "\n")
print("Close:\n", X_train_close, "\nAvg Volume:\n", X_train_avgVol, "\nVolatility:\n", 
      X_train_vola, "\nAvg Volatility:\n", X_train_avgVola, "\nCompany:\n", X_train_comp, "\nRise:\n", y_train_2)

print("--------------\n")

print("LENGTH OF VALIDATION SET:", len(y_valid_2), "\n")
print("Close:\n", X_valid_close, "\nAvg Volume:\n", X_valid_avgVol, "\nVolatility:\n", X_valid_vola, "\nAvg Volatility:\n", 
      X_valid_avgVola, "\nCompany:\n", X_valid_comp, "\nRise:\n", y_valid_2)

print("--------------\n")

print("LENGTH OF TEST SET:", len(y_test_2), "\n")
print("Close:\n", X_test_close, "\nAvg Volume:\n", X_test_avgVol, "\nVolatility:\n", 
      X_test_vola, "\nAvg Volatility:\n", X_test_avgVola, "\nCompany:\n", X_test_comp, "\nRise:\n", y_test_2)

#RNN
X_train_close_rnn = X_train_close
X_train_avgVol_rnn = X_train_avgVol
X_train_vola_rnn = X_train_vola
X_train_avgVola_rnn = X_train_avgVola 
X_train_comp_rnn = X_train_comp 
y_train_rnn = y_train_2

X_valid_close_rnn = X_valid_close
X_valid_avgVol_rnn = X_valid_avgVol
X_valid_vola_rnn = X_valid_vola
X_valid_avgVola_rnn = X_valid_avgVola 
X_valid_comp_rnn = X_valid_comp 
y_valid_rnn = y_valid_2

#RESHAPE THE INPUTS:
X_train_close_rnn = np.reshape(X_train_close_rnn, (X_train_close_rnn.shape[0], seq_len, 1)) #reshape input to be [# samples, # past time steps, # features]
X_train_avgVol_rnn = np.reshape(X_train_avgVol_rnn, (X_train_avgVol_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_train_vola_rnn = np.reshape(X_train_vola_rnn, (X_train_vola_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_train_avgVola_rnn = np.reshape(X_train_avgVola_rnn, (X_train_avgVola_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_train_comp_rnn = np.reshape(X_train_comp_rnn, (X_train_comp_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
y_train_rnn = np.reshape(y_train_rnn, (y_train_rnn.shape[0], 1, 1)) 

X_valid_close_rnn = np.reshape(X_valid_close_rnn, (X_valid_close_rnn.shape[0], seq_len, 1)) #reshape input to be [# samples, # past time steps, # features]
X_valid_avgVol_rnn = np.reshape(X_valid_avgVol_rnn, (X_valid_avgVol_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_valid_vola_rnn = np.reshape(X_valid_vola_rnn, (X_valid_vola_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_valid_avgVola_rnn = np.reshape(X_valid_avgVola_rnn, (X_valid_avgVola_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_valid_comp_rnn = np.reshape(X_valid_comp_rnn, (X_valid_comp_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
y_valid_rnn = np.reshape(y_valid_rnn, (y_valid_rnn.shape[0], 1, 1)) 

#TUNE THE HYPERPARAMETERS
  #batch size - number of randomly selected samples in each minibatch
B = [25, 35, 50] 
  #epochs - how many times to show the entire training set to the network
E = [25, 50, 100] 
  #neurons - number of neurons in LSTM layer and dense layer
N = [10, 20]
  #dropout - the ratio of the dropout layer
D = [0.2, 0.3]

checkpoints = []
cp_callbacks = []
for b in B:
  for e in E:
    for n in N:
      for d in D:
        #DECLARE CALLBACK
        checkpoints.append("model_" + datetime.datetime.now().strftime("%m-%d-%Y-%H-%M-%S"))
        cp_callbacks.append(ModelCheckpoint(
            filepath=checkpoints[-1],
            monitor='val_accuracy', 
            verbose=1, 
            save_best_only=True, 
            mode='max'))
        #ASSEMBLE THE MODEL:
        close_input = keras.Input(shape=(X_train_close_rnn.shape[1], X_train_close_rnn.shape[2]), name="close") 
        avgVol_input = keras.Input(shape=(X_train_avgVol_rnn.shape[1]), name ="avg_volume") 
        vola_input = keras.Input(shape=(X_train_vola_rnn.shape[1]), name="volatility") 
        avgVola_input = keras.Input(shape=(X_train_avgVola_rnn.shape[1]), name="avg_volatility") 
        comp_input = keras.Input(shape=(X_train_comp_rnn.shape[1]), name="company") 

        lstm_input = LSTM(n, name="LSTM", activation='relu')(close_input)
        concatenated = layers.concatenate([lstm_input, avgVol_input, vola_input, avgVola_input, comp_input])
        dense_input = Dense(n, name="DENSE", activation='relu')(concatenated)
        dropout_input = Dropout(d)(dense_input)
        dense_input2 = Dense(n, name="DENSE_2", activation='relu')(dropout_input)
        dense_input3 = Dense(5, name="DENSE_3", activation='relu') (dense_input2)
        out = Dense(1, name="Rise", activation='sigmoid')(dense_input3) 
        model = keras.Model(inputs=[close_input, avgVol_input, vola_input, avgVola_input, comp_input], outputs=out)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        
        #FIT THE MODEL WITH TRAINING DATA AND VALIDATION DATA
        model.fit({"close":X_train_close_rnn, "avg_volume": X_train_avgVol_rnn, "volatility":X_train_vola_rnn, 
                   "avg_volatility":X_train_avgVola_rnn, "company":X_train_comp_rnn}, {"Rise":y_train_rnn}, epochs=e, batch_size=b, verbose=2, 
                   callbacks=[cp_callbacks[-1]], validation_data=({"close":X_valid_close_rnn, "avg_volume": X_valid_avgVol_rnn, 
                   "volatility":X_valid_vola_rnn, "avg_volatility":X_valid_avgVola_rnn, "company":X_valid_comp_rnn}, {"Rise":y_valid_rnn}))

print("All Saved Callbacks:", checkpoints)

max_val_acc = cp_callbacks[0].best
best_callback = cp_callbacks[0] #initialize best_callback to the first one
for i in range(len(cp_callbacks)):
  if cp_callbacks[i].best > max_val_acc:
    max_val_acc = cp_callbacks[i].best #update the max_val_acc
    best_callback = cp_callbacks[i] #update the best_callback if it currently has the max_val_acc

best_callback_checkpoint_path = best_callback._write_filepath
print("Best Callback Name:", best_callback._write_filepath)
print("Best Callback Accuracy:", best_callback.best)

from google.colab import drive 
drive.mount('/content/gdrive', force_remount=True)

model.save(F'/content/gdrive/My Drive/Saved Models/{best_callback_checkpoint_path}.h5'.format())

#Load the saved model
#Now, you don't need to train again or save all the models again!!
model = keras.models.load_model('/content/gdrive/MyDrive/Saved Models/model_11-14-2022-22-01-11.h5')

X_test_close_rnn = X_test_close
X_test_avgVol_rnn = X_test_avgVol
X_test_vola_rnn = X_test_vola
X_test_avgVola_rnn = X_test_avgVola 
X_test_comp_rnn = X_test_comp 
y_test_rnn = y_test_2
X_test_close_rnn = np.reshape(X_test_close_rnn, (X_test_close_rnn.shape[0], seq_len, 1)) #reshape input to be [# samples, # past time steps, # features]
X_test_avgVol_rnn = np.reshape(X_test_avgVol_rnn, (X_test_avgVol_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_test_vola_rnn = np.reshape(X_test_vola_rnn, (X_test_vola_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_test_avgVola_rnn = np.reshape(X_test_avgVola_rnn, (X_test_avgVola_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_test_comp_rnn = np.reshape(X_test_comp_rnn, (X_test_comp_rnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]

#Predict 
predictions = model.predict({"close":X_test_close_rnn, "avg_volume": X_test_avgVol_rnn, "volatility":X_test_vola_rnn, 
                             "avg_volatility":X_test_avgVola_rnn, "company":X_test_comp_rnn}, verbose = 0).flatten()
preds = []
for p in predictions:
  if p > 0.50:
    preds.append(1)
  else:
    preds.append(0)
y_test_rnn = [int(x) for x in y_test_rnn] 
print("\nClassification Report:\n", classification_report(y_true=y_test_rnn, y_pred=preds, digits=4))
print("\t    RNN Confusion Matrix:")
cf_matrix = confusion_matrix(y_test_rnn, preds)
group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

#CNN
X_train_close_cnn = X_train_close
X_train_avgVol_cnn = X_train_avgVol
X_train_vola_cnn = X_train_vola
X_train_avgVola_cnn = X_train_avgVola 
X_train_comp_cnn = X_train_comp 
y_train_cnn = y_train_2

X_valid_close_cnn = X_valid_close
X_valid_avgVol_cnn = X_valid_avgVol
X_valid_vola_cnn = X_valid_vola
X_valid_avgVola_cnn = X_valid_avgVola 
X_valid_comp_cnn = X_valid_comp 
y_valid_cnn = y_valid_2

#RESHAPE THE INPUTS:
X_train_close_cnn = np.reshape(X_train_close_cnn, (X_train_close_cnn.shape[0], seq_len, 1)) #reshape input to be [# samples, # past time steps, # features]
X_train_avgVol_cnn = np.reshape(X_train_avgVol_cnn, (X_train_avgVol_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_train_vola_cnn = np.reshape(X_train_vola_cnn, (X_train_vola_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_train_avgVola_cnn = np.reshape(X_train_avgVola_cnn, (X_train_avgVola_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_train_comp_cnn = np.reshape(X_train_comp_cnn, (X_train_comp_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
y_train_cnn = np.reshape(y_train_cnn, (y_train_cnn.shape[0], 1, 1)) 

X_valid_close_cnn = np.reshape(X_valid_close_cnn, (X_valid_close_cnn.shape[0], seq_len, 1)) #reshape input to be [# samples, # past time steps, # features]
X_valid_avgVol_cnn = np.reshape(X_valid_avgVol_cnn, (X_valid_avgVol_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_valid_vola_cnn = np.reshape(X_valid_vola_cnn, (X_valid_vola_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_valid_avgVola_cnn = np.reshape(X_valid_avgVola_cnn, (X_valid_avgVola_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_valid_comp_cnn = np.reshape(X_valid_comp_cnn, (X_valid_comp_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
y_valid_cnn = np.reshape(y_valid_cnn, (y_valid_cnn.shape[0], 1, 1)) 

#specify the filter
n_filters = (8, 8, 8)

#TUNE THE HYPERPARAMETERS
  #batch size - number of randomly selected samples in each minibatch
B = [25, 35, 50] 
  #epochs - how many times to show the entire training set to the network
E = [25, 50, 100] 
  #neurons - number of neurons in DENSE layers
N = [10, 20]
  #dropout - the ratio of the dropout layer
D = [0.2, 0.3]

cnn_checkpoints = []
cnn_cp_callbacks = []
for b in B:
  for e in E:
    for n in N:
      for d in D:
        #DECLARE CALLBACK
        cnn_checkpoints.append("model_" + datetime.datetime.now().strftime("%m-%d-%Y-%H-%M-%S"))
        cnn_cp_callbacks.append(ModelCheckpoint(
            filepath=cnn_checkpoints[-1],
            monitor='val_accuracy', 
            verbose=1, 
            save_best_only=True, 
            mode='max'))
        #ASSEMBLE THE MODEL:      
        close_input = keras.Input(shape=(X_train_close_cnn.shape[1], X_train_close_cnn.shape[2]), name="close") 
        avgVol_input = keras.Input(shape=(X_train_avgVol_rnn.shape[1]), name ="avg_volume") 
        vola_input = keras.Input(shape=(X_train_vola_rnn.shape[1]), name="volatility") 
        avgVola_input = keras.Input(shape=(X_train_avgVola_rnn.shape[1]), name="avg_volatility") 
        comp_input = keras.Input(shape=(X_train_comp_cnn.shape[1]), name ="company") 

        cnn_1 = Conv1D(n_filters[0], kernel_size=2, name="CONV_1", activation='relu', input_shape=(seq_len, 1))(close_input)

        pool = MaxPooling1D(pool_size=2)(cnn_1)
        cnn_3 = Conv1D(n_filters[2], kernel_size=1, name="CONV_5", activation='relu')(pool)
        pool_2 = MaxPooling1D(pool_size=2)(cnn_3)
        flat = Flatten()(pool_2)
        drop = Dropout(d)(flat)
        concat = layers.concatenate([drop, avgVol_input, vola_input, avgVola_input, comp_input])
        dense = Dense(n, name="DENSE", activation='relu')(concat)
        drop_2 = Dropout(d)(dense)
        dense_2 = Dense(n, name="DENSE_2", activation='relu')(drop_2)
        dense_3 = Dense(5, name="DENSE_3", activation='relu') (dense_2)
        out = Dense(1, name="Rise", activation='sigmoid')(dense_3) 
        model = keras.Model(inputs=[close_input, avgVol_input, vola_input, avgVola_input, comp_input], outputs=out)
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        
        #FIT THE MODEL WITH TRAINING AND VALIDATION DATA        
        model.fit({"close":X_train_close_cnn, "avg_volume": X_train_avgVol_cnn, "volatility":X_train_vola_cnn, 
            "avg_volatility":X_train_avgVola_cnn, "company":X_train_comp_cnn}, {"Rise":y_train_cnn}, epochs=e, batch_size=b, verbose=2, 
            callbacks=[cnn_cp_callbacks[-1]], validation_data=({"close":X_valid_close_cnn, "avg_volume": X_valid_avgVol_cnn, 
            "volatility":X_valid_vola_cnn, "avg_volatility":X_valid_avgVola_cnn, "company":X_valid_comp_cnn}, {"Rise":y_valid_cnn}))

print("All Saved Callbacks:", cnn_checkpoints)

max_val_acc = cnn_cp_callbacks[0].best
cnn_best_callback = cnn_cp_callbacks[0] #initialize best_callback to the first one
for i in range(len(cnn_cp_callbacks)):
  if cnn_cp_callbacks[i].best > max_val_acc:
    max_val_acc = cnn_cp_callbacks[i].best #update the max_val_acc
    cnn_best_callback = cnn_cp_callbacks[i] #update the best_callback if it currently has the max_val_acc

cnn_best_callback_checkpoint_path = cnn_best_callback._write_filepath
print("Best Callback Name:", cnn_best_callback._write_filepath)
print("Best Callback Accuracy:", cnn_best_callback.best)

model.save(F'/content/gdrive/My Drive/Saved Models/{cnn_best_callback_checkpoint_path}.h5'.format())

#Load the saved model
#Now, you don't need to train again or save all the models again!!
model = keras.models.load_model('/content/gdrive/MyDrive/Saved Models/model_11-14-2022-22-48-26.h5')

X_test_close_cnn = X_test_close
X_test_avgVol_cnn = X_test_avgVol
X_test_vola_cnn = X_test_vola
X_test_avgVola_cnn = X_test_avgVola 
X_test_comp_cnn = X_test_comp 
y_test_cnn = y_test_2
X_test_close_cnn = np.reshape(X_test_close_cnn, (X_test_close_cnn.shape[0], seq_len, 1)) #reshape input to be [# samples, # past time steps, # features]
X_test_avgVol_cnn = np.reshape(X_test_avgVol_cnn, (X_test_avgVol_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_test_vola_cnn = np.reshape(X_test_vola_cnn, (X_test_vola_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_test_avgVola_cnn = np.reshape(X_test_avgVola_cnn, (X_test_avgVola_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
X_test_comp_cnn = np.reshape(X_test_comp_cnn, (X_test_comp_cnn.shape[0], 1, 1)) #reshape input to be [# samples, # past time steps, # features]
y_test_cnn = np.reshape(y_test_cnn, (y_test_cnn.shape[0], 1, 1)) 

#Predict 
predictions_2 = model.predict({"close":X_test_close_cnn, "avg_volume": X_test_avgVol_cnn, "volatility":X_test_vola_cnn, 
                             "avg_volatility":X_test_avgVola_cnn, "company":X_test_comp_cnn}, verbose = 0).flatten()
preds_2 = []
for p in predictions_2:
  if p > 0.50:
    preds_2.append(1)
  else:
    preds_2.append(0)
y_test_cnn = [int(x) for x in y_test_cnn] 
print("\nClassification Report:\n", classification_report(y_true=y_test_cnn, y_pred=preds_2, digits=4))
print("\t    CNN Confusion Matrix:")
cf_matrix = confusion_matrix(y_test_cnn, preds_2)
group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

#FFNN
#Append the company information to the end of the window sequences of the closing prices
X_train_close_ffnn = X_train_close
X_train_avgVol_ffnn = X_train_avgVol
X_train_vola_ffnn = X_train_vola
X_train_avgVola_ffnn = X_train_avgVola 
X_train_comp_ffnn = X_train_comp 
y_train_ffnn = y_train_2

X_valid_close_ffnn = X_valid_close
X_valid_avgVol_ffnn = X_valid_avgVol
X_valid_vola_ffnn = X_valid_vola
X_valid_avgVola_ffnn = X_valid_avgVola 
X_valid_comp_ffnn = X_valid_comp 
y_valid_ffnn = y_valid_2

X_train_avgVol_ffnn = X_train_avgVol_ffnn.reshape(-1, 1)
X_valid_avgVol_ffnn = X_valid_avgVol_ffnn.reshape(-1, 1)

X_train_vola_ffnn = X_train_vola_ffnn.reshape(-1, 1)
X_valid_vola_ffnn = X_valid_vola_ffnn.reshape(-1, 1)

X_train_avgVola_ffnn = X_train_avgVola_ffnn.reshape(-1, 1)
X_valid_avgVola_ffnn = X_valid_avgVola_ffnn.reshape(-1, 1)

X_train_comp_ffnn = X_train_comp_ffnn.reshape(-1, 1)
X_valid_comp_ffnn = X_valid_comp_ffnn.reshape(-1, 1)

#TUNE THE HYPERPARAMETERS
  #batch size - number of randomly selected samples in each minibatch
B = [25, 35, 50] 
  #epochs - how many times to show the entire training set to the network
E = [25, 50, 100] 
  #neurons - number of neurons in DENSE layers
N = [5, 10, 30]

ffnn_checkpoints = []
ffnn_cp_callbacks = []
#Train the model with different hyperparameters
for b in B:
  for e in E:
    for n in N:
      #DECLARE CALLBACK
      ffnn_checkpoints.append("model_" + datetime.datetime.now().strftime("%m-%d-%Y-%H-%M-%S"))
      ffnn_cp_callbacks.append(ModelCheckpoint(
          filepath=ffnn_checkpoints[-1],
          monitor='val_accuracy', 
          verbose=1, 
          save_best_only=True, 
          mode='max'))
      #ASSEMBLE THE MODEL
      close_input = keras.Input(shape=(X_train_close_ffnn.shape[1],), name="close") #shape is window size
      avgVol_input = keras.Input(shape=(X_train_avgVol_ffnn.shape[1],), name="avg_volume") 
      vola_input = keras.Input(shape=(X_train_vola_ffnn.shape[1],), name="volatility") 
      avgVola_input = keras.Input(shape=(X_train_avgVola_ffnn.shape[1],), name="avg_volatility") 
      comp_input = keras.Input(shape=(X_train_comp_ffnn.shape[1],), name="company") 

      concat = layers.concatenate([close_input, avgVol_input, vola_input, avgVola_input, comp_input])

      dense_1 = Dense(n, activation='relu', input_dim=seq_len+1)(concat)
      dense_2 = Dense(n, activation='relu')(dense_1)   
      out = Dense(1, name="Rise", activation='sigmoid')(dense_2)    
      model = keras.Model(inputs=[close_input, avgVol_input, vola_input, avgVola_input, comp_input], outputs=out)
      model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
          
      #FIT THE MODEL WITH TRAINING DATA AND VALIDATION DATA
      model.fit({"close":X_train_close_ffnn, "avg_volume": X_train_avgVol_ffnn, "volatility":X_train_vola_ffnn, 
            "avg_volatility":X_train_avgVola_ffnn, "company":X_train_comp_ffnn}, {"Rise":y_train_ffnn}, epochs=e, batch_size=b, verbose=2, 
            callbacks=[ffnn_cp_callbacks[-1]], validation_data=({"close":X_valid_close_ffnn, "avg_volume": X_valid_avgVol_ffnn, 
            "volatility":X_valid_vola_ffnn, "avg_volatility":X_valid_avgVola_ffnn, "company":X_valid_comp_ffnn}, {"Rise":y_valid_ffnn}))

print("All Saved Callbacks:", ffnn_checkpoints)

max_val_acc = ffnn_cp_callbacks[0].best
ffnn_best_callback = ffnn_cp_callbacks[0] #initialize best_callback to the first one
for i in range(len(ffnn_cp_callbacks)):
  if ffnn_cp_callbacks[i].best > max_val_acc:
    max_val_acc = ffnn_cp_callbacks[i].best #update the max_val_acc
    ffnn_best_callback = ffnn_cp_callbacks[i] #update the best_callback if it currently has the max_val_acc

ffnn_best_callback_checkpoint_path = ffnn_best_callback._write_filepath
print("Best Callback Name:", ffnn_best_callback._write_filepath)
print("Best Callback Accuracy:", ffnn_best_callback.best)

model.save(F'/content/gdrive/My Drive/Saved Models/{ffnn_best_callback_checkpoint_path}.h5'.format())

#Load the saved model
#Now, you don't need to train again or save all the models again!!
model = keras.models.load_model('/content/gdrive/MyDrive/Saved Models/model_11-14-2022-23-28-20.h5')

X_test_close_ffnn = X_test_close
X_test_avgVol_ffnn = X_test_avgVol
X_test_vola_ffnn = X_test_vola
X_test_avgVola_ffnn = X_test_avgVola 
X_test_comp_ffnn = X_test_comp 
y_test_ffnn = y_test_2

X_test_avgVol_ffnn = X_test_avgVol_ffnn.reshape(-1, 1)
X_test_vola_ffnn = X_test_vola_ffnn.reshape(-1, 1)
X_test_avgVola_ffnn = X_test_avgVola_ffnn.reshape(-1, 1)
X_test_comp_ffnn = X_test_comp_ffnn.reshape(-1, 1)

#Predict
predictions_3 = model.predict({"close":X_test_close_ffnn, "avg_volume": X_test_avgVol_ffnn, "volatility":X_test_vola_ffnn, 
                             "avg_volatility":X_test_avgVola_ffnn, "company":X_test_comp_ffnn}, verbose = 0).flatten()
preds_3 = []
for p in predictions_3:
  if p > 0.50:
    preds_3.append(1)
  else:
    preds_3.append(0)
print("\nClassification Report:\n", classification_report(y_true=y_test_ffnn, y_pred=preds_3, digits=4))
print("\t    FFNN Confusion Matrix:")
cf_matrix = confusion_matrix(y_test_ffnn, preds_3)
group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')