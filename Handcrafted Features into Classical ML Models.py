# -*- coding: utf-8 -*-
"""Capstone_Assignment_14_handcrafted_nonsequential.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NSQIS_L5eT9YWsfl20F5YDdbTUsYDN8g
"""

'''
Task: Stock market prediction with classification -> Predict whether the prices will rise or fall in the next day.
Companies: Apple and Procter & Gamble
Models: SVM, KNN, DT, LR, XGB Classifier
Features: use historical stock market data to calculate handcrafted features such as Average Volume, Volatility, Average Volatility, and Company
Implementation:
  1) Prepare the dataframe
  2) Train the models
  3) Make predictions
'''

#Load the dataset
!pip install yfinance

#Import statements
import yfinance as yf
import pandas as pd
import matplotlib.pyplot as plt
import math
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split 
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report as creport
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix as cmatrix
import numpy as np
import warnings
warnings.filterwarnings("ignore")
from sklearn.metrics import plot_confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.layers import Flatten
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.preprocessing import MinMaxScaler
from numpy.random import seed
from tensorflow import random
seed(123)
random.set_seed(123)
from datetime import timedelta
import datetime
from xgboost import XGBClassifier
from sklearn.utils import resample
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from sklearn.metrics import confusion_matrix

###########################
#Data Preprocessing
###########################
#Split the data into train, validation, and test sets
#64% train, 16% validation, 20% test
def split_data(data, companies):
  #Declare the data sets
  train_data = pd.DataFrame()
  valid_data = pd.DataFrame()
  test_data = pd.DataFrame()

  #Calculate the values of where to split the data
  tot_len = len(data)
  train_len = int(tot_len * 0.64) 
  valid_len = train_len + int(tot_len * 0.16)

  #Get the train, validation, and test data, then randomly shuffle it
  train_data = data.iloc[0:train_len]
  valid_data = data.iloc[train_len:valid_len]
  test_data = data.iloc[valid_len:]

  train_data = shuffle(train_data)
  valid_data = shuffle(valid_data)
  test_data = shuffle(test_data)

  #Now split into X and y
  #The feature columns are the X data, and the label column is the y data
  X_train = pd.DataFrame()
  y_train = pd.DataFrame()
  X_valid = pd.DataFrame()
  y_valid = pd.DataFrame()
  X_test = pd.DataFrame()
  y_test = pd.DataFrame()

  X_train = train_data[['Avg Volume', 'Volatility', 'Avg Volatility', 'Company']]
  y_train = train_data['Rise']

  X_valid = valid_data[['Avg Volume', 'Volatility', 'Avg Volatility', 'Company']]
  y_valid = valid_data['Rise']

  X_test = test_data[['Avg Volume', 'Volatility', 'Avg Volatility', 'Company']]
  y_test = test_data['Rise']

  return X_train, y_train, X_valid, y_valid, X_test, y_test
#########################################################################################################################
#Normalize the data - Avg Volume is the only one that needs to be normalized
def normalize_data(train, valid, test):
  #Create temporary dataframes for the train and valid datasets
  train_norm = pd.DataFrame()
  train_norm['Avg Volume'] = train['Avg Volume']

  valid_norm = pd.DataFrame()
  valid_norm['Avg Volume'] = valid['Avg Volume']

  test_norm = pd.DataFrame()
  test_norm['Avg Volume'] = test['Avg Volume']

  #define scaler
  scaler = StandardScaler()
  #fit only on the train dataset
  scaler = scaler.fit(train_norm) 

  #transform the train dataset
  train_norm = scaler.transform(train_norm)
  #Put the transformed data into a new dataframe column
  train_norm = list(train_norm)
  tn = pd.DataFrame()
  tn['Avg Volume'] = train_norm

  #transform the valid dataset with the scaler that is fitted with the train dataset
  valid_norm = scaler.transform(valid_norm)
  #Put the transformed data into a new dataframe column
  valid_norm = list(valid_norm)
  vn = pd.DataFrame()
  vn['Avg Volume'] = valid_norm

  #transform the test dataset with the scaler that is fitted with the train dataset
  test_norm = scaler.transform(test_norm)
  #Put the transformed data into a new dataframe column
  test_norm = list(test_norm)
  ten = pd.DataFrame()
  ten['Avg Volume'] = test_norm

  #Put the normalized data into the original dataframes
  train['Avg Volume'] = tn['Avg Volume'].values
  valid['Avg Volume'] = vn['Avg Volume'].values
  test['Avg Volume'] = ten['Avg Volume'].values

  train['Avg Volume'] = train['Avg Volume'].str.get(0)
  valid['Avg Volume'] = valid['Avg Volume'].str.get(0)
  test['Avg Volume'] = test['Avg Volume'].str.get(0)

  return train, valid, test
#########################################################################################################################
#Calculate average volume for a feature
def averageVolume(df_col):
  volumes = list(df_col)

  stock_volume = list()
  s_vol = 0
  prices = list(df_col)
  for i in range(len(prices)): #First range parameter is 6 because we don't want to include 2018-01-01
    #average volume over the previous 5 days
    s_vol = volumes[i-5] + volumes[i-4] + volumes[i-3] + volumes[i-2] + volumes[i-1]
    s_vol = s_vol / 5
    stock_volume.append(s_vol)

  return stock_volume
#########################################################################################################################
#Calculate volatilty for a feature
def calc_volatility(df_col):
  prices = list(df_col)
  volatility = list()
  vol = 0

  for i in range(len(prices)):
    vol = prices[i-1] - prices[i]
    vol = vol / prices[i-1]
    volatility.append(vol)
    
  return volatility
#########################################################################################################################
#Calculate Stock Price Volatility for a feature- It is an average of previous 5 days volatility of a company. 
def calc_stock_price_volatility(df_col):
  prices = list(df_col)
  volatility = calc_volatility(df_col)

  stock_price_volatility = list()
  sp_vol = 0

  for i in range(len(prices)): 
    sp_vol = volatility[i-5] + volatility[i-4] + volatility[i-3] + volatility[i-2] + volatility[i-1]
    sp_vol = sp_vol / 5
    stock_price_volatility.append(sp_vol)

  return stock_price_volatility
#########################################################################################################################
#Function to get the previous 5 days from the start date in order to calculate the handcrafted features
def get_adjusted_start_date(sd, interv, days_before):
  x = sd.split('-')
  year = int(x[0])
  month = int(x[1])
  day = int(x[2])
  x = datetime.datetime(year, month, day)
  if(interv == '1d'):
    date = x - timedelta(days=days_before)
  elif(interv == '1wk'):
    date = x - timedelta(weeks=days_before)
  else:
    date = x - timedelta(months=days_before)

  date = str(date)
  date = date.split(' ')
  date = date[0]

  return date
#########################################################################################################################
#Function to get one day in the future from the end date in order to calculate the labels
def get_adjusted_end_date(sd, interv, days_ahead):
  x = sd.split('-')
  year = int(x[0])
  month = int(x[1])
  day = int(x[2])
  x = datetime.datetime(year, month, day)
  if(interv == '1d'):
    date = x + timedelta(days=days_ahead)
  elif(interv == '1wk'):
    date = x + timedelta(weeks=days_ahead)
  else:
    date = x + timedelta(months=days_ahead)

  date = str(date)
  date = date.split(' ')
  date = date[0]

  return date
#########################################################################################################################
#This function takes the arguments that the user passed for the companies, start date, end date, and interval
#But gets 5 datapoints before the start date and 1 datapoint before the end date.
#This is because we need the extra datapoints in order to calculated the handcrafted features and the labels.
#These newly added datapoints will get thrown out after the features and labels are calculated.
def get_adjusted_dataframe(companies, sd, ed, interv):
  #Download the historical data based on the parameters
  data = yf.download(tickers=companies, start=sd, end=ed, interval=interv)

  #Download one future day as well
  orig_len = len(data)
  days_ahead = 1
  new_ed = ed
  while(len(data) < orig_len + 1):
    data = pd.DataFrame()
    new_ed = get_adjusted_end_date(new_ed, interv, days_ahead)
    data = yf.download(tickers=companies, start=sd, end=new_ed, interval=interv)

  #Download the previous 5 days as well
  orig_len = len(data)
  days_before = 1
  new_sd = sd
  while(len(data) < orig_len + 5): 
    data = pd.DataFrame()
    new_sd = get_adjusted_start_date(new_sd, interv, days_before)
    data = yf.download(tickers=companies, start=new_sd, end=new_ed, interval=interv)

  return data
#########################################################################################################################
#This function calculates the 'Company' feature and the 'Rise' label.
#It also counts how many rise and fall data points there are.
def get_comp_and_lab(data, companies):
  #Declare lists to store all the information
  company = list()
  label = list()
  coms = list()
  lab = list()

  #This variable will distinguish the companies from each other
  j = 0

  #These variables will let us know which class is the majority and minority
  rise = 0
  fall = 0

  for ticker in companies:
    data = data.drop(columns=[('Adj Close', ticker), ('High', ticker), ('Low', ticker), ('Open', ticker)])

    #Create the label column and the company column
    coms = list()
    lab = list()
    y = list(data[('Close', ticker)].pct_change())
    y.pop(0)
    for i in range(len(y)):
      num = str(y[i]) #get the percent change as a string in order to tell if it is negative or positive.
      coms.append(j) #save the company
      if num[0] == '-':
        lab.append(0) #save the label, 0 = Fall
        fall += 1 #sum of the number of fall data points
      elif num[0] != '-':
        lab.append(1) #save the label, 1 = Rise
        rise += 1 #sum of the number of rise data points
    lab.append(np.nan) #append nan to the end of labels because we are going to drop the last day from the dataframe
    coms.append(j) #append one more to the end of coms so that the length is the same as the lab length (we will drop last element of each company anyway)
    j += 1
    data[('Company', ticker)] = coms
    data[('Rise', ticker)] = lab

  return data, rise, fall
#########################################################################################################################
def get_more_features(data, companies):
  for ticker in companies:
    volatility = list()
    stock_price_volatility = list()
    avg_volume = list()

    #Volatility
    df_col = data[('Close', ticker)]
    volatility = calc_volatility(df_col)
    #Stock Price Volatility
    stock_price_volatility = calc_stock_price_volatility(data[('Close', ticker)])
    #Average Stock Volume
    avg_volume = averageVolume(data[('Volume', ticker)])

    data[('Avg Volume', ticker)] = avg_volume
    data[('Volatility', ticker)] = volatility
    data[('Avg Volatility', ticker)] = stock_price_volatility

  return data
#########################################################################################################################
#This function combines both tickers so that the different pieces of information from each company can be in one 
#column in the dataframe, rather than each company have it's own columns
def combine_tickers(data, companies):
  avg_volume = list()
  volatility = list()
  avg_volatility = list()
  company = list()
  rise = list()
  idx = list()

  for ticker in companies:
    #Get a list of the date and ticker so that we can rename the index of the dataframe
    for i in range(len(data.index)):
      new_str = str(data.index[i])
      new_new_str = new_str.split(' ')
      new_index = new_new_str[0] + "_" + ticker
      idx.append(new_index)
  
    #Save the information in lists that will be added to a new dataframe
    for i in data[('Avg Volume', ticker)]:
      avg_volume.append(i) #Save all the average volumes
    for i in data[('Volatility', ticker)]:
      volatility.append(i) #Save all the volatilities
    for i in data[('Avg Volatility', ticker)]:
      avg_volatility.append(i) #Save all the average volatilities  
    for i in data[('Company', ticker)]:
      company.append(i)
    for i in data[('Rise', ticker)]:
      rise.append(i)

  #Create a new dataframe
  df = pd.DataFrame()
  df['Avg Volume'] = avg_volume
  df['Volatility'] = volatility
  df['Avg Volatility'] = avg_volatility
  df['Company'] = company
  df['Rise'] = rise

  #rename the index numbers to be the date/company
  for i in range(len(df)):
    df.rename(index={i:idx[i]}, inplace=True)

  return df
#########################################################################################################################
#This function drops the excess fall or rise data points in the data set so that there are an equal number of both
def downsample_data(data, rise, fall, companies):
  #Downsample the majority class if the split is greater than 51%-49% so that there will be an equal 
  #(or close to equal) number of Fall and Rise data points
  df_new = pd.DataFrame()
  tot_samples = rise + fall
  if ((rise/tot_samples) > 0.51):
    if (rise < fall):
      df_min = data[data.Rise == 1]
      df_maj = data[data.Rise == 0]
    else:
      df_min = data[data.Rise == 0]
      df_maj = data[data.Rise == 1]

    df_maj_down = resample(df_maj, replace = False, n_samples = len(df_min), random_state = 0)
    df_new = pd.concat([df_min,df_maj_down])
  else:
    df_new = data

  df_new = df_new.sort_index()

  return df_new
#########################################################################################################################
#Fetch the historical stock market data from the yfinance module and manipulate it into handcrafted features
#You can adjust: companies, start date, end date, and time interval
def get_Features(companies, sd, ed, interv):
  #Create an empty dataframe
  data = pd.DataFrame()
  data = get_adjusted_dataframe(companies, sd, ed, interv)
  #Drop the empty cells in the dataframe
  data = data.dropna()
  #Get average volume, volatility and average volatility
  data = get_more_features(data, companies)
  #Get the information for the company feature and the labels and also count how many rise and fall data points there are
  data, rise, fall = get_comp_and_lab(data, companies)

  #Delete the last row
  data = data.drop(data.index[-1])
  #Delete the first 5 rows
  for i in range(5):
    data = data.drop(data.index[0])

  #Drop Close and Volume columns
  for ticker in companies:
    data = data.drop(columns=[('Close', ticker), ('Volume', ticker)])

  #Make one column for each feature rather than having seperate columns for each company
  data = combine_tickers(data, companies)
  #Downsample the data to have a balanced number of rise and fall data points
  data = downsample_data(data, rise, fall, companies)

  #64% train, 16% validation, 20% test
  X_train = pd.DataFrame()
  y_train = pd.DataFrame()
  X_valid = pd.DataFrame()
  y_valid = pd.DataFrame()
  X_test = pd.DataFrame()
  y_test = pd.DataFrame()
  X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(data, companies)

  # #Normalize average volume
  # X_train, X_valid, X_test = normalize_data(X_train, X_valid, X_test)

  #Return the newly created dataframe and train/valid/test sets
  return data, X_train, y_train, X_valid, y_valid, X_test, y_test
#########################################################################################################################
#Function to assemble the final dataframe
def getData(companies, sd, ed, interv):
  data = pd.DataFrame()
  X_train = pd.DataFrame()
  y_train = pd.DataFrame()
  X_valid = pd.DataFrame()
  y_valid = pd.DataFrame()
  X_test = pd.DataFrame()
  y_test = pd.DataFrame()
  data, X_train, y_train, X_valid, y_valid, X_test, y_test = get_Features(companies, sd, ed, interv)
  return data, X_train, y_train, X_valid, y_valid, X_test, y_test

companies = ['AAPL', 'PG']
data, X_train, y_train, X_valid, y_valid, X_test, y_test = getData(companies, '1999-12-31', '2020-01-02', "1d")

print("--------dataframe--------\n")
print(data)
print("--------X_train--------\n")
print(X_train)
print("--------y_train--------\n")
print(y_train)
print("--------X_valid--------\n")
print(X_valid)
print("--------y_valid--------\n")
print(y_valid)
print("--------X_test--------\n")
print(X_test)
print("--------y_test--------\n")
print(y_test)
print("-----------------------\n")
print("# of data set samples:", len(data))
print("# of train set samples:", len(X_train))
print("# of valid set samples:", len(X_valid))
print("# of test set samples:", len(X_test))

###########################
#Train the Models
###########################

#SVM
def trainSVM(X_train_svc, y_train_svc, X_valid_svc, y_valid_svc, X_test_svc, y_test_svc):

  P = [ 'l1', 'l2', 'elasticnet', 'none' ]
  L = [ 'hinge', 'squared_hinge' ]
  C = [1 , 10 , 0.1]

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for p in P: 
    for c in C:
      for l in L:
        try:
          svc_clf = LinearSVC(penalty=p, C=c, loss=l, random_state=123)
          svc_clf.fit(X_train_svc, y_train_svc)
          pred_svc = svc_clf.predict(X_valid_svc)
          results[(p, c, l)] = f1_score(y_valid_svc, pred_svc, average='weighted')
        except:
          continue

  print("SVC Model:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if max_value < current_value:
      max_value = current_value
      p_best = item[0][0]
      c_best = item[0][1]
      l_best = item[0][2]

  print("Best hyperparameters:", p_best, ",", c_best, ",", l_best)

  #Train the model with the best parameters
  svc_clf = LinearSVC(penalty=p_best, C=c_best, loss=l_best, random_state=123)
  svc_clf.fit(X_train_svc, y_train_svc)
  #Do prediction with original test set, not the validation set
  y_pred_svc = svc_clf.predict(X_test_svc)
  print("\nClassification Report:\n", classification_report(y_true=y_test_svc, y_pred=y_pred_svc, digits=4))
  print("\t    SVM Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_svc, y_pred_svc)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainSVM(X_train, y_train, X_valid, y_valid, X_test, y_test)

#KNN
def trainKNN(X_train_knn, y_train_knn, X_valid_knn, y_valid_knn, X_test_knn, y_test_knn):
  N = list(range(1, 12, 2))
  M = ['chebyshev', 'manhattan', 'euclidean']

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for n in N:
    for m in M:
      knn_clf = KNeighborsClassifier(n_neighbors=n, metric=m)
      knn_clf.fit(X_train_knn, y_train_knn)
      pred_knn = knn_clf.predict(X_valid_knn)
      results[(n, m)] = f1_score(y_valid_knn, pred_knn, average='weighted')

  print("KNN Model:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if max_value < current_value:
      max_value = current_value
      n_best = item[0][0]
      m_best = item[0][1]

  print("Best hyperparameters:", n_best,",", m_best)

  #Train the model with the best parameters
  knn_clf = KNeighborsClassifier(n_neighbors=n_best, metric=m_best)
  knn_clf.fit(X_train_knn, y_train_knn)
  #Do prediction with original test set, not the validation set
  y_pred_knn = knn_clf.predict(X_test_knn)
  print("\nClassification Report:\n", classification_report(y_true=y_test_knn, y_pred=y_pred_knn, digits=4))
  print("\t    KNN Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_knn, y_pred_knn)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainKNN(X_train, y_train, X_valid, y_valid, X_test, y_test)

#DT
def trainDT(X_train_dt, y_train_dt, X_valid_dt, y_valid_dt, X_test_dt, y_test_dt):
  C = ['gini', 'entropy']

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for c in C:
      dt_clf = DecisionTreeClassifier(criterion=c) #do I need random_state=123?
      dt_clf.fit(X_train_dt, y_train_dt)
      pred_dt = dt_clf.predict(X_valid_dt)
      results[c] = f1_score(y_valid_dt, pred_dt, average='weighted')

  print("DT Model:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if (max_value < current_value) or (max_value == current_value):
      max_value = current_value
      c_best = item[0]

  print("Best hyperparameter:", c_best)

  # #Train the model with the best parameters
  dt_clf = DecisionTreeClassifier(criterion=c_best) #do I need random_state=123?
  dt_clf.fit(X_train_dt, y_train_dt)
  #Do prediction with original test set, not the validation set
  y_pred_dt = dt_clf.predict(X_test_dt)
  print("\nClassification Report:\n", classification_report(y_true=y_test_dt, y_pred=y_pred_dt, digits=4))
  print("\t    DT Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_dt, y_pred_dt)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainDT(X_train, y_train, X_valid, y_valid, X_test, y_test)

#LR
def trainLR(X_train_lr, y_train_lr, X_valid_lr, y_valid_lr, X_test_lr, y_test_lr):    
  P = [ 'l1', 'l2', 'elasticnet', 'none' ]
  S = [ 'newton-cg', 'lbfgs', 'sag', 'liblinear', 'saga' ]
  C = [1 , 10 , 0.1]

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for p in P:
    for c in C:
      for s in S:
        try:
          lr_clf = LogisticRegression(penalty=p, C=c, solver=s, random_state=123)
          lr_clf.fit(X_train_lr, y_train_lr)
          pred_lr = lr_clf.predict(X_valid_lr)
          results[(p, c, s)] = f1_score(y_valid_lr, pred_lr, average='weighted')
        except:
          continue

  print("LR Model of:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if max_value < current_value:
      max_value = current_value
      p_best = item[0][0]
      c_best = item[0][1]
      s_best = item[0][2]
      
  print("Best hyperparameters:", p_best, ",", c_best, ",", s_best)

  #Train the model with the best parameters
  lr_clf = LogisticRegression(penalty=p_best, C=c_best, solver=s_best, random_state=123)
  lr_clf.fit(X_train_lr, y_train_lr)
  #Do prediction with original test set, not the validation set
  y_pred_lr = lr_clf.predict(X_test_lr)
  print("\nClassification Report:\n", classification_report(y_true=y_test_lr, y_pred=y_pred_lr, digits=4))
  print("\t    LR Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_lr, y_pred_lr)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainLR(X_train, y_train, X_valid, y_valid, X_test, y_test)

#XGB
def trainXGB(X_train_xgb, y_train_xgb, X_valid_xgb, y_valid_xgb, X_test_xgb, y_test_xgb):
  B = ['gbtree', 'gblinear', 'dart'] #booster options
  O = ['reg:squarederror', 'binary:hinge', 'reg:logistic', 'binary:logistic']#Objective defines the loss function to be minimized
  N = [100, 1000] #n_estimators
  L = [0.02, 0.01] #learning rate

  #Train the model with different hyperparameters and store the scores in a dictionary
  results = {}
  for b in B:
    for o in O:
      for n in N:
        for l in L:
          xgb_clf = XGBClassifier(booster=b, objective=o, n_estimators=n, learning_rate=l)
          xgb_clf.fit(X_train_xgb, y_train_xgb)
          pred_xgb = xgb_clf.predict(X_valid_xgb)
          results[(b, o, n, l)] = f1_score(y_valid_xgb, pred_xgb, average='weighted')

  print("XGB Classifier Model:\n")
  #Get best results and print classification report
  max_value = 0.0
  for item in results.items():
    current_value = item[1]
    if max_value < current_value:
      max_value = current_value
      b_best = item[0][0]
      o_best = item[0][1]
      n_best = item[0][2]
      l_best = item[0][3]

  print("Best hyperparameters:", b_best, ",", o_best, ",", n_best, ",", l_best)

  # #Train the model with the best parameters
  xgb_clf = XGBClassifier(booster=b_best, objective=o_best, n_estimators=n_best, learning_rate=l_best) 
  xgb_clf.fit(X_train_xgb, y_train_xgb)
  #Do prediction with original test set, not the validation set
  y_pred_xgb = xgb_clf.predict(X_test_xgb)
  print("\nClassification Report:\n", classification_report(y_true=y_test_xgb, y_pred=y_pred_xgb, digits=4))
  print("\t    XGB Confusion Matrix:")
  cf_matrix = confusion_matrix(y_test_xgb, y_pred_xgb)
  group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_counts)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='PiYG')

trainXGB(X_train, y_train, X_valid, y_valid, X_test, y_test)